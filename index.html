<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset"utf-8" />
        <meta name"viewport" content"width=device-width, initial-scale1" />
        <title>
            Inspect: An Open Source Framework for LLM Evaluations
        </title>
        <link rel="stylesheet"
            href="tufte.css" />
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/stackoverflow-dark.min.css"
            media="(prefers-color-scheme: dark)" />
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/stackoverflow-light.min.css"
            media="(prefers-color-scheme: light)" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/
highlight.min.js">
    </script>
        <script>
        hljs.highlightAll();
    </script>
        <style>
        :root {
            --background-color: #f9f9f9;
            --text-color: #111;
            --link-color: #0000ee;
            --visited-link-color: #551a8b;
            --code-bg: #f0f0f0;
            --code-color: #333;
            --blockquote-border: #ccc;
            --highlight-bg: #fff8dc;
            --warning-bg: #ffe6e6;
            --note-bg: #e6f3ff;
            --table-header-bg: #f5f5f5;
            --table-border: #ddd;
        }

        @media (prefers-color-scheme: dark) {
            :root {
                --background-color: #151515;
                --text-color: #e0e0e0;
                --link-color: #8ab4f8;
                --visited-link-color: #c58af9;
                --code-bg: #2d2d2d;
                --code-color: #e0e0e0;
                --blockquote-border: #555;
                --highlight-bg: #3a3a2c;
                --warning-bg: #3d2929;
                --note-bg: #25344d;
                --table-header-bg: #2d2d2d;
                --table-border: #444;
            }
        }

        body {
            max-width: 1400px;
            margin: 0 auto;
            counter-reset: sidenote-counter;
            background-color: var(--background-color);
            color: var(--text-color);
        }

        section {
            padding-bottom: 0;
        }

        p {
            width: 60% !important;
        }

        h1,
        h2,
        h3,
        p,
        li,
        table {
            color: var(--text-color);
        }

        h3 {
            font-style: normal;
            margin-top: 3rem;
        }

        a {
            color: var(--link-color);
        }

        a:visited {
            color: var(--visited-link-color);
        }

        .subtitle {
            font-style: italic;
            margin-top: -1.5rem;
            margin-bottom: 2rem;
        }

        .author {
            margin-bottom: 0.5rem;
        }

        .date {
            margin-bottom: 2rem;
        }

        .code-block code {
            border-radius: 5px;
        }

        .sidenote code {
            font-size: 0.8em !important;
        }

        img {
            max-width: 100%;
        }

        figure {
            margin: 2rem 0;
        }

        figcaption {
            font-size: 0.9rem;
            font-style: italic;
            text-align: center;
            margin-top: 0.5rem;
        }

        blockquote {
            border-left: 3px solid var(--blockquote-border);
            padding-left: 1rem;
            margin-left: 0;
            font-style: italic;
        }

        .highlight {
            background-color: var(--highlight-bg);
            padding: 0.5rem 1rem;
            border-radius: 5px;
            margin: 1.5rem 0;
        }

        .warning {
            background-color: var(--warning-bg);
            padding: 0.5rem 1rem;
            border-radius: 5px;
            margin: 1.5rem 0;
        }

        .note {
            background-color: var(--note-bg);
            padding: 0.5rem 1rem;
            border-radius: 5px;
            margin: 1.5rem 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 2rem 0;
        }

        th,
        td {
            border: 1px solid var(--table-border);
            padding: 0.5rem;
        }

        th {
            background-color: var(--table-header-bg);
        }

        .sidenote,
        .marginnote {
            color: var(--text-color);
            opacity: 0.8;
        }

        @media (max-width: 760px) {
            pre {
                font-size: 0.8em;
            }
        }
    </style>
    </head>

    <body>
        <article>
            <h1>
                Inspect: An Open Source Framework for LLM Evaluations
            </h1>
            <p class="subtitle">
                A systematic approach to testing language models
            </p>
            <p class="author">
                Michael Li • March 22, 2025
            </p>
            <section>
                <h2>Introduction</h2>
                <p>
                    As language models become more capable and widely deployed,
                    the need for robust evaluation frameworks has
                    grown. Evaluating these models presents unique challenges:
                    they require testing across many dimensions
                    (accuracy, helpfulness, safety), their capabilities evolve
                    rapidly, and their behaviors can be difficult to
                    quantify.
                    <a href="https://inspect.ai-safety-institute.org.uk/"
                        target="_blank">
                        Inspect
                    </a>
                    is an open source framework from the UK AI Security
                    Institute (
                    <a
                        href="https://www.politico.eu/article/jd-vance-britain-ai-safety-institute-aisi-security/"
                        target="_blank">
                        formerly
                    </a> the UK AI Safety Institute)<label for="sn-institute"
                        class="margin-toggle sidenote-number">
                    </label>
                    <input type="checkbox" id="sn-institute"
                        class="margin-toggle" />
                    <span class="sidenote">
                        Nothing ominous about this whatsoever.
                    </span>
                    which aims to address these challenges.
                </p>
                <p>
                    The UK AI Security Institute is a directorate of the
                    Department of Science, Innovation, and Technology that
                    conducts rigorous research to enable advanced AI governance.
                    Notably, they conducted pre-deployment evaluations of
                    frontier AI models like <a
                        href="https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-openais-o1-model"
                        target="_blank">OpenAI's O1</a> and <a
                        href="https://www.aisi.gov.uk/work/pre-deployment-evaluation-of-anthropics-upgraded-claude-3-5-sonnet"
                        target="_blank">Anthropic's Claude 3.5 Sonnet</a> in
                    collaboration with the US AI Safety Institute<label
                        for="sn-us-aisi"
                        class="margin-toggle sidenote-number"></label>
                    <input type="checkbox" id="sn-us-aisi"
                        class="margin-toggle" />
                    <span class="sidenote">
                        The US AI Safety Institute was created by an executive
                        order from former President Joe Biden, which President
                        Trump repealed on his first day back in office. It is
                        also expecting <a
                            href="https://techcrunch.com/2025/02/22/us-ai-safety-institute-could-face-big-cuts/"
                            target="_blank">mass layoffs</a> as part of
                        Elon Musks' DOGE initiative.
                    </span>
                    testing capabilities across domains such as cybersecurity,
                    biological research, and software development.
                </p>
                <p>
                    Traditional evaluation methods for LLMs often suffer from
                    fragmentation and inaccessibility. Evaluations tend to be
                    insular, frequently propped up by custom internal tooling
                    and disparate code notebooks. This lack of standardization
                    makes it difficult to compare results across models or
                    research teams, while the absence of shared
                    infrastructure forces researchers to repeatedly reinvent
                    evaluation methodologies. Additionally, as models
                    increasingly incorporate external tools and autonomous
                    capabilities, the complexity of proper evaluation grows
                    beyond what ad-hoc approaches can effectively measure.
                </p>
                <p>
                    Inspect provides a flexible, extensible framework for
                    creating evaluations that test LLMs more thoroughly.
                    <a
                        href="https://www.gov.uk/government/news/ai-safety-institute-releases-new-ai-safety-evaluations-platform"
                        target="_blank">Released</a>
                    in May 2024, it has since gained adoption among
                    researchers and engineers working on model
                    evaluation. In this post, I'll explain how Inspect works,
                    demonstrate it with a practical movie
                    recommendation example, and discuss its strengths and
                    limitations.
                </p>
            </section>
            <section>
                <h2>What Is Inspect?</h2>
                <p>
                    Inspect is a Python framework built specifically for
                    evaluating language models. At its core, Inspect
                    provides a systematic approach to testing LLMs across
                    multiple dimensions such as accuracy, reasoning
                    ability, and safety.
                </p>
                <p>
                    The framework follows a modular architecture with components
                    that work together to create flexible and
                    powerful evaluations:
                </p>
                <h3>Core Components</h3>
                <p>
                    <b>Datasets</b> contain the test examples that models will
                    be evaluated on. Each example (or "sample")
                    typically includes an input prompt and an expected output or
                    "target." Inspect supports various data formats
                    including CSV, JSON, and Hugging Face datasets.
                </p>
                <p>
                    <b>Solvers</b> process the inputs and produce outputs.
                    They define how to interact with the model – from
                    simple prompting to chain-of-thought to complex multi-turn
                    dialogues. Solvers can be chained together to
                    implement sophisticated evaluation strategies.
                </p>
                <p>
                    <b>Scorers</b> evaluate the model outputs against expected
                    targets. This might involve exact matching, fuzzy
                    comparison, model-graded evaluation, or custom scoring
                    logic. Scorers produce metrics that summarize model
                    performance.
                </p>
                <p>
                    <b>Tasks</b> are the fundamental unit of evaluation in
                    Inspect. A task brings everything together by
                    combining a dataset with solvers and scorers to define a
                    complete evaluation. Tasks can include options for
                    parallel execution, error handling, and environment
                    configuration.
                </p>
                <p>
                    Here's a simple example of defining a task in Inspect:
                </p>
                <div class="code-block">
                    <pre class="python"><code>from inspect_ai import Task, task
from inspect_ai.dataset import example_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import generate, system_message

@task
def simple_qa():
    return Task(
        dataset=example_dataset("simple_qa"),
        solver=[
            system_message("You are a helpful assistant."),
            generate()
        ],
        scorer=model_graded_fact()
    )</code></pre>
                </div>
                <p>
                    This code defines a task that:
                </p>
                <ol>
                    <li>Uses a dataset of QA pairs</li>
                    <li>Applies a system message to orient the model</li>
                    <li>Generates responses from the model</li>
                    <li>
                        Scores the responses using another model as a judge
                    </li>
                </ol>
                <p>
                    Inspect supports many model providers including OpenAI,
                    Anthropic, Google, and others. It handles parallel
                    evaluation, provides detailed logging, supports tool-using
                    agents, includes secure sandboxing for code
                    execution, and has a VSCode extension for a better developer
                    experience.
                </p>
            </section>
            <section>
                <h2>Using Inspect for Movie Recommendations</h2>
                <p>
                    Suppose we are Netflix, in the not-so-distant future, and
                    we've offloaded our entire recommendation system to
                    a single LLM.
                    <label for="sn-netflix"
                        class="margin-toggle sidenote-number">
                    </label>
                    <input type="checkbox" id="sn-netflix"
                        class="margin-toggle" />
                    <span class="sidenote">
                        I originally made this up, but it <br />turns out
                        this is actually
                        <a href="https://research.netflix.com/publication/
large-language-models-as-zero-shot-conversational-recommenders" target="_blank">
                            real
                        </a>.
                    </span>
                    Let's use Inspect to evaluate our LLM on its movie
                    recommendations.
                </p>
                <p>
                    First, we'll install Inspect.
                </p>
                <div class="code-block">
                    <pre
                        class="bash"><code>!pip install inspect-ai</code></pre>
                </div>
                <p>
                    Next, we'll create a dataset of movie recommendation
                    scenarios. In a real-world setting, we might draw these
                    from user queries and known good recommendations, but for
                    this example, we'll create them manually. Each
                    sample contains a user query about movie recommendations and
                    an expected target that represents ideal
                    recommendations.
                </p>
                <div class="code-block">
                    <pre class="python"><code>import json
from inspect_ai.dataset import Sample, MemoryDataset

# Define our test cases
recommendation_samples = [
    Sample(
        input="I liked 'The Dark Knight' and 'Inception'. What similar movies would I like?",
        target="Christopher Nolan films like Interstellar, Oppenheimer or The Prestige",
        metadata={"category": "director_based"}
    ),
    Sample(
        input="I'm looking for feel good documentaries similar to 'Won't You Be My Neighbor'",
        target="Positive documentaries like 'RBG', 'Jiro Dreams of Sushi' or 'Crip Camp'",
        metadata={"category": "genre_specific"}
    ),
    Sample(
        input="What are some good movies to watch with my 8-year-old child?",
        target="Family-friendly animated films like 'How to Train Your Dragon', 'Big Hero 6' or 'The Mitchells vs the Machines'",
        metadata={"category": "audience_specific"}
    )
]

# Create a dataset
recommendation_dataset = MemoryDataset(recommendation_samples)

# Optionally save to a file for reuse
with open("movie_recommendations.json", "w") as f:
    json.dump([s.__dict__ for s in recommendation_samples], f)</code></pre>
                </div>
                <p>
                    Now, let's define a system message to orient the model to
                    its role as a movie recommendation system. This will
                    help set the context for the model's responses:
                </p>
                <div class="code-block">
                    <pre class="python"><code>SYSTEM_MESSAGE = """
You are an expert movie recommendation system. Your job is to provide
relevant movie recommendations based on the user's preferences. Consider
genre, director, themes, and style when making recommendations. Be specific
and suggest actual movie titles that the user might enjoy based on their
stated preferences.
"""</code></pre>
                </div>
                <p>
                    Next, we'll create a custom scorer that checks for multiple
                    criteria in the recommendations. This will give
                    us a more nuanced evaluation than a simple text match:
                </p>
                <div class="code-block">
                    <pre
                        class="python"><code>from inspect_ai.model import get_model
from inspect_ai.scorer import Score, Target, TaskState, accuracy, scorer, stderr

@scorer(metrics=[accuracy(), stderr()])
def recommendation_quality():
    """Score recommendations on relevance, diversity, and explanation."""
    
    SCORER_TEMPLATE = """
Evaluate the following movie recommendation response against the ideal
target. Score each criterion on a scale of 0-2:

- Relevance: Are the recommended movies similar to the user's preferences?
  (0-2)
- Diversity: Does the response provide a variety of options? (0-2)
- Explanation: Does the response explain why these movies were recommended?
  (0-2)

User request: {request}
Model response: {response}
Ideal response: {target}

For each criterion, provide a score and brief justification.
Then provide a final overall score as "GRADE: C" for correct (total >= 4)
or "GRADE: I" for incorrect.
"""
    
    async def score(state: TaskState, target: Target):
        prompt = SCORER_TEMPLATE.format(
            request=state.input_text,
            response=state.output.completion,
            target=target.text
        )
        
        result = await get_model().generate(prompt)
        
        # Extract grade (C or I) from response
        if "GRADE: C" in result.completion:
            value = "C"
        else:
            value = "I"
            
        return Score(
            value=value,
            answer=state.output.completion,
            explanation=result.completion
        )
    
    return score</code></pre>
                </div>
                <p>
                    This scorer uses another model to grade the recommendations.
                    It evaluates relevance, diversity, and whether
                    the response provides explanations. The grader model looks
                    at both the model's output and the expected
                    target, then makes a judgment about quality.
                </p>
                <p>
                    Now let's create our evaluation task. We'll define multiple
                    approaches to test different prompting strategies
                    like chain-of-thought:
                    <label for="sn-approaches"
                        class="margin-toggle sidenote-number">
                    </label>
                    <input type="checkbox" id="sn-approaches"
                        class="margin-toggle" />
                    <span class="sidenote">
                        Reasoning is in vogue, so we're going to use it. For
                        more information on the built-in solvers (like
                        <code>system_message()</code>,
                        <code>chain_of_thought()</code>,
                        and <code>self_critique()</code>), check out the
                        <a href="https://inspect.aisi.org.uk/solvers.html"
                            target="_blank">
                            documentation
                        </a>.
                    </span>
                </p>
                <div class="code-block">
                    <pre class="python"><code>from inspect_ai import Task, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import chain_of_thought, generate, system_message

@task
def movie_recommendation_eval(approach="basic"):
    """Evaluate movie recommendation quality using different approaches.
    
    Args:
        approach: Either "basic", "cot" (chain of thought), or "custom_score"
    """
    if approach == "basic":
        solver = [system_message(SYSTEM_MESSAGE), generate()]
        scorer = model_graded_fact()
    elif approach == "cot":
        solver = [
            system_message(SYSTEM_MESSAGE),
            chain_of_thought(),
            generate()
        ]
        scorer = model_graded_fact()
    elif approach == "custom_score":
        solver = [system_message(SYSTEM_MESSAGE), generate()]
        scorer = recommendation_quality()
    else:
        raise ValueError(f"Unknown approach: {approach}")
    
    return Task(
        dataset=json_dataset("movie_recommendations.json"),
        solver=solver,
        scorer=scorer
    )</code></pre>
                </div>
                <p>
                    This task definition allows us to compare different
                    approaches. We can also add a tool-using capability to our
                    evaluation. This would allow the model to look up additional
                    information about movies when making
                    recommendations:
                </p>
                <div class="code-block">
                    <pre
                        class="python"><code>from inspect_ai.solver import use_tools
from inspect_ai.tool import tool

@tool
def movie_database():
    """Tool to query information about movies."""
    
    async def execute(movie_name: str):
        """
        Query information about movies.

        Args:
            movie_name (str): The name of the movie to look up.

        Returns:
            dict: A dictionary containing movie information or an error message if not found.
        """
        # These movies are obtained from the ones in the movie API
        # provided by the instructors
        movies = {
            "The Dark Knight": {
                "year": 2008,
                "director": "Christopher Nolan",
                "genre": ["Action", "Crime", "Drama"],
                "rating": 9.0
            },
            "Inception": {
                "year": 2010,
                "director": "Christopher Nolan",
                "genre": ["Action", "Adventure", "Sci-Fi"],
                "rating": 8.8
            }
            # More movies would be defined here...
        }
        
        # Simple fuzzy matching
        for name, info in movies.items():
            if movie_name.lower() in name.lower():
                return {
                    "name": name,
                    "info": info
                }
        
        return {"error": f"Movie '{movie_name}' not found in database"}
    
    return execute

@task
def tool_using_recommender():
    """Evaluation task for a recommendation system that uses tools."""
    
    return Task(
        dataset=json_dataset("movie_recommendations.json"),
        solver=[
            system_message(SYSTEM_MESSAGE),
            use_tools([movie_database()]),
            generate()
        ],
        scorer=recommendation_quality()
    )</code></pre>
                </div>
                <p>
                    This adds a tool that allows the model to query a (mock)
                    movie database. In an actual implementation, this
                    could connect to an actual database or API to get
                    information about movies.
                </p>
                <p>
                    Finally, let's run our evaluations and analyze the results.
                    We'll use Mistral 7B Instruct
                    <label for="sn-mistral"
                        class="margin-toggle sidenote-number"></label>
                    <input type="checkbox" id="sn-mistral"
                        class="margin-toggle" />
                    <span class="sidenote">
                        This choice is a solely economic one. I only <br>
                        get $1.00 in free credits from Together AI, and <br>
                        Mistral 7B Instruct is $0.20/1M tokens.
                    </span>
                    via the Together AI API for this example:
                </p>
                <div class="code-block">
                    <pre class="python"><code>from inspect_ai import eval
                  
eval(movie_recommendation_eval(approach="basic"),
    model="together/mistralai/Mistral-7B-Instruct-v0.2")

eval(movie_recommendation_eval(approach="cot"),
    model="together/mistralai/Mistral-7B-Instruct-v0.2")

eval(movie_recommendation_eval(approach="custom_score"),
    model="together/mistralai/Mistral-7B-Instruct-v0.2")

eval(tool_using_recommender(),
    model="together/mistralai/Mistral-7B-Instruct-v0.2")

# The results are logged to the /logs folder</code></pre>
                </div>
                <p>
                    Inspect will execute each evaluation, call the models, and
                    compute metrics.
                    The results are logged comprehensively, allowing us to
                    analyze performance in detail.
                </p>
                <p>
                    Inspect provides a built-in log viewer for exploring and
                    analyzing results.
                    You can start it with:
                </p>
                <div class="code-block">
                    <pre class="bash"><code>inspect view</code></pre>
                </div>
                <p>
                    This opens a web interface that shows detailed information
                    about each evaluation, including model inputs and outputs,
                    scoring decisions, and performance metrics. This view is
                    very detailed and more informatoin can be found
                    <a href="https://inspect.aisi.org.uk/log-viewer.html"
                        target="_blank">here</a>.
                </p>
                <figure>
                    <label for="mn-demo"
                        class="margin-toggle">&#8853;</label>
                    <input type="checkbox" id="mn-demo" class="margin-toggle" />
                    <span class="marginnote">
                        Here's what the main view of an evaluation log looks
                        like in the Inspect dashboard.
                    </span>
                    <img
                        src="dashboard.png"
                        alt="Inspect Dashboard Evaluation Log">
                </figure>
                <figure>
                    <label for="mn-demo"
                        class="margin-toggle">&#8853;</label>
                    <input type="checkbox" id="mn-demo" class="margin-toggle" />
                    <span class="marginnote">
                        And here's what the log history view looks like. The
                        full logs for this run can be found in the <a
                            href="https://github.com/ml5885/inspect_blog_post"
                            target="_blank">Github repo</a>.
                    </span>
                    <img
                        src="dashboard2.png"
                        alt="Inspect Dashboard Evaluation Log History">
                </figure>
                <p>
                    This example only touches the surface of what's possible
                    with Inspect. The framework provides many more capabilities,
                    including parallel execution of evaluations
                    across multiple models, caching to reduce API costs,
                    integration with CI/CD pipelines,
                    and support for complex agent evaluations.
                    You can learn more in the <a
                        href="https://inspect.aisi.org.uk/"
                        target="_blank">official documentation</a>.
                </p>
                <p>
                    The UK AI Security Institute also maintains a collection of
                    community-contributed evaluations using the Inspect
                    framework called
                    <a href="https://ukgovernmentbeis.github.io/inspect_evals/"
                        target="_blank">Inspect Evals</a>,
                    which includes benchmarks for coding (<a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/coding/swe_bench/"
                        target="_blank">SWE-Bench</a>,
                    <a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/coding/scicode/"
                        target="_blank">SciCode</a>),
                    agents (<a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/assistants/osworld/"
                        target="_blank">OSWorld</a>,
                    <a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/assistants/sycophancy/"
                        target="_blank">Sycophancy Eval</a>),
                    cybersecurity (<a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/cybersecurity/cybench/"
                        target="_blank">CyBench</a>,
                    <a
                        href="https://ukgovernmentbeis.github.io/inspect_evals/evals/cybersecurity/intercode_ctf/"
                        target="_blank">InterCode:
                        CTF</a>),
                    and more. Looking through them is pretty interesting, and
                    gives a sense of what the frontier evaluation landscape
                    looks like.
                </p>
                <section>
                    <h2>Strengths and Limitations</h2>
                    <h3>Strengths</h3>
                    <p>
                        <b>Comprehensive logging and visualization:</b> Inspect
                        has detailed logging that tracks all parts of an
                        evaluation. The log viewer is really good - it shows
                        everything in a clear web interface. This makes it easy
                        to see how models perform and fix problems.
                    </p>
                    <p>
                        <b>Support for complex evaluations:</b> The framework
                        provides implementations for complex tasks like
                        multi-step reasoning, chain-of-thought, and tool use.
                    </p>
                    <p>
                        <b>Sandboxed code execution:</b> While we didn't use it
                        in our example, Inspect lets you run code safely in a
                        protected environment. This helps when testing models
                        that generate code that might be risky.
                    </p>
                    <p>
                        <b>Flexible modular architecture:</b> Inspect is built
                        with separate parts (datasets, solvers, scorers, tasks)
                        that work together. You can mix and match these parts to
                        create custom tests without starting from scratch each
                        time.
                    </p>

                    <h3>Limitations</h3>
                    <p>
                        <b>Model-dependent reliability:</b> The framework relies
                        on models that aren't perfect. In our movie example, one
                        evaluation was scored wrong because the judge model gave
                        a "GRADE: B" when it should have used "GRADE: C" or
                        "GRADE: I". These kinds of mistakes show how using
                        models to judge other models can be unreliable.
                    </p>
                    <p>
                        <b>Learning curve and complexity:</b> The framework is
                        flexible but also complex. New users might find it hard
                        to learn. Setting up good evaluations means
                        understanding how all the parts work together and how to
                        write effective prompts.
                    </p>
                    <p>
                        <b>Adaptability challenges:</b> Even though Inspect
                        tries to be flexible, the future of AI is unpredictable.
                        As new types of models and capabilities emerge, the
                        framework will need to keep changing to stay useful.
                        What works for today's models might not work for
                        tomorrow's.
                    </p>
                </section>
                <section>
                    <h2>Conclusion</h2>
                    <p>
                        Inspect is a Python framework for evaluating language
                        models that provides components for datasets, solvers,
                        scorers, and task definitions. It enables systematic
                        testing across various dimensions of model performance.
                    </p>
                    <p>
                        It's interesting to see a government institution like
                        the UK AI Security Institute developing and
                        open-sourcing an evaluation framework. The field of LLM
                        evaluation is rapidly developing, and frameworks like
                        Inspect will likely continue to adapt as new model
                        capabilities emerge and evaluation methodologies mature.
                    </p>
                    <p>
                        If you're interested in what the frontier of LLM
                        evaluations looks like, especially in the context of
                        safety, I recommend
                        <a href="https://www.apolloresearch.ai/blog"
                            target="_blank">reading</a>
                        <a
                            href="https://arxiv.org/abs/2212.09251"
                            target="_blank">these</a>
                        <a href="https://arxiv.org/abs/2403.13793"
                            target="_blank">articles</a>.
                    </p>
                    <p>
                        <a
                            href="https://github.com/ml5885/inspect_blog_post/discussions"
                            target="_blank">Here</a> is the Github Discussion
                        page for this post.
                    </p>
                </section>
                <section></section>
                <footer>
                    <p>
                        Built with HTML and
                        <a href="https://edwardtufte.github.io/tufte-css/"
                            target="_blank">
                            Tufte CSS
                        </a>
                    </p>
                </footer>
            </article>
        </body>

    </html>
